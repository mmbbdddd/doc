
1，要开发理解面向对象思想是有点门槛的，主要是对悟性有要求

2，日常开发中只需要强调在灵活多变的场景中使用规则引擎，脚本引擎等技术解决问题即可

3，规则引擎，脚本引擎解决点状的变化；流程编排解决线状的变化

4，对象抽象的方式上是门槛最高的

    比如我们批量风控的抽象是FRD，是产品，规则，日期的结合，而不分别是F，R，D抽象成三个概念，或者他们彼此的组合。

    我总结的经验
    1，需要一些个抽象，对上能提供产品，规则，交易数据的扩展性
    2，对下，能屏蔽数据加载，内存占用，线程调度管理等问题
    3，2的问题，在LDTP架构中，无论FRD，还是F R D没有本质区别。但是在批量架构中，只有FRD彼此的结合才是有意义
    4，2的抽象，符合DDD中无需关注how，只需关注what的哲学。是最直接的，最简单的。

    在spark的核心抽象中，RDD的抽象————形式上是一个数据集的stream语法，和Java的List流语法上没有本质区别。
            但在关系上，RDD维护了数据集和资源（主机资源）之间的灵活映射关系，使得成为一个透明的分布式架构。

    同样的，RDD无需关注特定的存储结构，，无需关注特定的Map或者Reduce的计算模式，RDD的数据和计算模式如何扩展
    ，生成，那是另外一个次要层面的问题。

    这也是spark很容易扩展成几千个计算算子，而几万节点，而同时保持了极清晰简洁的架构、极高性能的原因。



